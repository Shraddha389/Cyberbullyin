{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea313baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../scripts/')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb037b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# encoders\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# models\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "# metrics\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics import recall_score, roc_auc_score\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# local scripts\n",
    "from text_utils import preprocess_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37bc5718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38153, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(9539, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load train, validation and test sets into dataframe\n",
    "df_train = pd.read_csv('../data/train_data.csv')\n",
    "df_valid = pd.read_csv('../data/valid_data.csv')\n",
    "df_test = pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "# combine train and validation sets\n",
    "# shuffle dataframe randomly\n",
    "df_train = pd.concat([df_train, df_valid]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# shape of train and test sets: (rows, columns)\n",
    "display(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccac5ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smh... “@tayyoung_: FUCK OBAMA, dumb ass nigger”</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If one retired army officer does something bru...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acting like the nazis didn’t take inspiration ...</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tje clps on peel scholl dont care abt cyber bu...</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#parents Another key word 4 Bullying Preventio...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0   Smh... “@tayyoung_: FUCK OBAMA, dumb ass nigger”          ethnicity\n",
       "1  If one retired army officer does something bru...           religion\n",
       "2  acting like the nazis didn’t take inspiration ...                age\n",
       "3  Tje clps on peel scholl dont care abt cyber bu...                age\n",
       "4  #parents Another key word 4 Bullying Preventio...  not_cyberbullying"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7095c72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Smh... “@tayyoung_: FUCK OBAMA, dumb ass nigger”</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If one retired army officer does something bru...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acting like the nazis didn’t take inspiration ...</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tje clps on peel scholl dont care abt cyber bu...</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#parents Another key word 4 Bullying Preventio...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0   Smh... “@tayyoung_: FUCK OBAMA, dumb ass nigger”          ethnicity\n",
       "1  If one retired army officer does something bru...           religion\n",
       "2  acting like the nazis didn’t take inspiration ...                age\n",
       "3  Tje clps on peel scholl dont care abt cyber bu...                age\n",
       "4  #parents Another key word 4 Bullying Preventio...  not_cyberbullying"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Goree_JuhssGuns hahaha he ain't even worth my...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @hsaymssik: Sucks to have the smile wiped o...</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just a reminder, it's absolutely disgusting to...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @BuzzFeedUK: When you accidentally open you...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Loving the look of the fritters! #mkr</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text   cyberbullying_type\n",
       "0  @Goree_JuhssGuns hahaha he ain't even worth my...            ethnicity\n",
       "1  RT @hsaymssik: Sucks to have the smile wiped o...               gender\n",
       "2  Just a reminder, it's absolutely disgusting to...            ethnicity\n",
       "3  RT @BuzzFeedUK: When you accidentally open you...  other_cyberbullying\n",
       "4              Loving the look of the fritters! #mkr    not_cyberbullying"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first 5 datapoints of train and test sets\n",
    "display(df_train.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d810bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38153,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(9539,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract independent features\n",
    "# preprocess text column\n",
    "X_train = preprocess_corpus(df_train.tweet_text)\n",
    "X_test = preprocess_corpus(df_test.tweet_text)\n",
    "\n",
    "# size of train & test sets\n",
    "display(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "177009a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  smh tayyoung fuck obama dumb nigger\n",
       "1    one retired army officer something brutal whol...\n",
       "2    act like nazi take inspiration gas chamber use...\n",
       "3    tje clps peel scholl dont care abt cyber bully...\n",
       "4    parent another key word bully prevention think...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    goree juhssguns hahaha even worth tweet dumb f...\n",
       "1    hsaymssik suck smile wiped face huh kat glass ...\n",
       "2    reminder absolutely disgust see people would p...\n",
       "3            buzzfeeduk accidentally open front camera\n",
       "4                                love look fritter mkr\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first 5 preprocessed tweets of train & test sets\n",
    "display(X_train.head())\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f892ae8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38153, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(9539, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encode the class labels\n",
    "# extract dependent features\n",
    "y_train = df_train.cyberbullying_type\n",
    "y_test = df_test.cyberbullying_type\n",
    "\n",
    "# spawn a label encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# train the encoder on the train set labels\n",
    "encoder.fit(y_train.values.ravel())\n",
    "\n",
    "# transform the labels\n",
    "y_train = pd.DataFrame(encoder.transform(y_train.values.ravel()), columns=['cyberbullying_type'])\n",
    "y_test = pd.DataFrame(encoder.transform(y_test.values.ravel()), columns=['cyberbullying_type'])\n",
    "\n",
    "# size of train & test set class labels\n",
    "display(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b94a7d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cyberbullying_type\n",
       "0                   1\n",
       "1                   5\n",
       "2                   0\n",
       "3                   0\n",
       "4                   3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cyberbullying_type\n",
       "0                   1\n",
       "1                   2\n",
       "2                   1\n",
       "3                   4\n",
       "4                   3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first 5 encoded class labels of train & test sets\n",
    "display(y_train.head())\n",
    "display(y_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d27e93",
   "metadata": {},
   "source": [
    "# Bag of Words Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "532740e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38153, 3192)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(9539, 3192)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bag of words transformation\n",
    "# instantiate a CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(min_df=15)\n",
    "\n",
    "# train and construct bag of words\n",
    "X_train_bow = pd.DataFrame(bow_vectorizer.fit_transform(X_train).toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "X_test_bow = pd.DataFrame(bow_vectorizer.transform(X_test).toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "\n",
    "# shape of document matrix: (rows, columns)\n",
    "display(X_train_bow.shape, X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e375759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aalwuhaib</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abt</th>\n",
       "      <th>abu</th>\n",
       "      <th>abuse</th>\n",
       "      <th>...</th>\n",
       "      <th>yousufpoosuf</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>ypg</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yup</th>\n",
       "      <th>zaibatsunews</th>\n",
       "      <th>zappe</th>\n",
       "      <th>zero</th>\n",
       "      <th>zionist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aalwuhaib  abc  ability  able  abortion  absolute  absolutely  abt  abu  \\\n",
       "0          0    0        0     0         0         0           0    0    0   \n",
       "1          0    0        0     0         0         0           0    0    0   \n",
       "2          0    0        0     0         0         0           0    0    0   \n",
       "3          0    0        0     0         0         0           0    1    0   \n",
       "4          0    0        0     0         0         0           0    0    0   \n",
       "\n",
       "   abuse  ...  yousufpoosuf  youth  youtube  ypg  yrs  yup  zaibatsunews  \\\n",
       "0      0  ...             0      0        0    0    0    0             0   \n",
       "1      0  ...             0      0        0    0    0    0             0   \n",
       "2      0  ...             0      0        0    0    0    0             0   \n",
       "3      0  ...             0      0        0    0    0    0             0   \n",
       "4      0  ...             0      0        0    0    0    0             0   \n",
       "\n",
       "   zappe  zero  zionist  \n",
       "0      0     0        0  \n",
       "1      0     0        0  \n",
       "2      0     0        0  \n",
       "3      0     0        0  \n",
       "4      0     0        0  \n",
       "\n",
       "[5 rows x 3192 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aalwuhaib</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abt</th>\n",
       "      <th>abu</th>\n",
       "      <th>abuse</th>\n",
       "      <th>...</th>\n",
       "      <th>yousufpoosuf</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>ypg</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yup</th>\n",
       "      <th>zaibatsunews</th>\n",
       "      <th>zappe</th>\n",
       "      <th>zero</th>\n",
       "      <th>zionist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aalwuhaib  abc  ability  able  abortion  absolute  absolutely  abt  abu  \\\n",
       "0          0    0        0     0         0         0           0    0    0   \n",
       "1          0    0        0     0         0         0           0    0    0   \n",
       "2          0    0        0     0         0         0           1    0    0   \n",
       "3          0    0        0     0         0         0           0    0    0   \n",
       "4          0    0        0     0         0         0           0    0    0   \n",
       "\n",
       "   abuse  ...  yousufpoosuf  youth  youtube  ypg  yrs  yup  zaibatsunews  \\\n",
       "0      0  ...             0      0        0    0    0    0             0   \n",
       "1      0  ...             0      0        0    0    0    0             0   \n",
       "2      0  ...             0      0        0    0    0    0             0   \n",
       "3      0  ...             0      0        0    0    0    0             0   \n",
       "4      0  ...             0      0        0    0    0    0             0   \n",
       "\n",
       "   zappe  zero  zionist  \n",
       "0      0     0        0  \n",
       "1      0     0        0  \n",
       "2      0     0        0  \n",
       "3      0     0        0  \n",
       "4      0     0        0  \n",
       "\n",
       "[5 rows x 3192 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first 5 datapoints of transformed train & validation sets\n",
    "display(X_train_bow.head())\n",
    "display(X_test_bow.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75c9cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "# plot confusion matrix using sns heatmap\n",
    "def plot_confusion_matrix(cf_matrix, title, xlabel='Predicted', ylabel='Actual', ticklabels=None, figsize=(8,4), fontdict={'fontsize':12}):\n",
    "    \n",
    "    # extract counts from confusion matrix\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "\n",
    "    # calculate proportions from confusion matrix\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    \n",
    "    # create annotations for plot\n",
    "    annotations = [f'{count}\\n{percentage}\\n'.format(count, percentage) for count, percentage in zip(group_counts, group_percentages)]\n",
    "    annotations = np.asarray(annotations).reshape(cf_matrix.shape)\n",
    "\n",
    "    plt.figure(figsize = figsize, dpi=100)\n",
    "    ax = sns.heatmap(cf_matrix, annot=annotations, fmt='', cmap='Blues')\n",
    "\n",
    "    # set title\n",
    "    ax.set_title(title, fontdict=fontdict)\n",
    "    \n",
    "    # set axes labels\n",
    "    ax.set_xlabel(xlabel, fontdict=fontdict)\n",
    "    ax.set_ylabel(ylabel, fontdict=fontdict)\n",
    "\n",
    "    # tick labels - List must be in alphabetical order\n",
    "    if ticklabels:\n",
    "        # set axes tick labels\n",
    "        ax.xaxis.set_ticklabels(ticklabels)\n",
    "        ax.yaxis.set_ticklabels(ticklabels)\n",
    "\n",
    "    # display the visualization of the confusion matrix.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f3e0e02",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'abc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\Dev\\languages\\python\\python310\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\Dev\\languages\\python\\python310\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32m~\\Dev\\languages\\python\\python310\\lib\\site-packages\\pandas\\core\\indexes\\range.py:389\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_loc(key, method\u001b[38;5;241m=\u001b[39mmethod, tolerance\u001b[38;5;241m=\u001b[39mtolerance)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'abc'"
     ]
    }
   ],
   "source": [
    "X_train['abc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d6268",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afe55fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.03, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.03, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.03, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.03, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.03, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END learning_rate=0.1, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=4, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=4, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=4, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=6, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=6, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=6, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=8, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=8, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=8, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=10, n_estimators=50;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=10, n_estimators=100;, score=nan total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.3, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.3, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.3, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.3, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.3, max_depth=10, n_estimators=500;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 180 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n180 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\core.py\", line 532, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 1382, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 401, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 1396, in <lambda>\n    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\core.py\", line 532, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\core.py\", line 643, in __init__\n    handle, feature_names, feature_types = dispatch_data_backend(\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\data.py\", line 899, in dispatch_data_backend\n    return _from_pandas_series(\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\data.py\", line 392, in _from_pandas_series\n    _invalid_dataframe_dtype(data)\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\data.py\", line 247, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category.  When\ncategorical type is supplied, DMatrix parameter `enable_categorical` must\nbe set to `True`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m grid_search_cv \u001b[38;5;241m=\u001b[39m GridSearchCV(XGBClassifier(), params, scoring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# perform grid search\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mgrid_search_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dev\\languages\\python\\python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\Dev\\languages\\python\\python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Dev\\languages\\python\\python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    849\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    850\u001b[0m     )\n\u001b[1;32m--> 852\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\Dev\\languages\\python\\python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 180 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n180 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\core.py\", line 532, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 1382, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 401, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\sklearn.py\", line 1396, in <lambda>\n    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\core.py\", line 532, in inner_f\n    return f(**kwargs)\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\core.py\", line 643, in __init__\n    handle, feature_names, feature_types = dispatch_data_backend(\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\data.py\", line 899, in dispatch_data_backend\n    return _from_pandas_series(\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\data.py\", line 392, in _from_pandas_series\n    _invalid_dataframe_dtype(data)\n  File \"C:\\Users\\ashut\\Dev\\languages\\python\\python310\\lib\\site-packages\\xgboost\\data.py\", line 247, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category.  When\ncategorical type is supplied, DMatrix parameter `enable_categorical` must\nbe set to `True`.\n"
     ]
    }
   ],
   "source": [
    "# create a map of params to be optimized\n",
    "params = {\n",
    "    'learning_rate': (0.03, 0.1, 0.3),\n",
    "    'max_depth': (4, 6, 8, 10),\n",
    "    'n_estimators': (50, 100, 500)\n",
    "}\n",
    "\n",
    "# instantiate a GridSearchCV object with SVM model and params\n",
    "grid_search_cv = GridSearchCV(XGBClassifier(), params, scoring = 'roc_auc', verbose=4, cv=5)\n",
    "\n",
    "# perform grid search\n",
    "grid_search_cv.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18359a04",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dtypes'"
     ]
    }
   ],
   "source": [
    "y_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5117871b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                 6442\n",
       "religion               6432\n",
       "age                    6389\n",
       "ethnicity              6358\n",
       "not_cyberbullying      6321\n",
       "other_cyberbullying    6211\n",
       "Name: cyberbullying_type, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
